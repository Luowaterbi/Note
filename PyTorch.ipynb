{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c7fae9",
   "metadata": {},
   "source": [
    "## Torch\n",
    "\n",
    "### Torch.topk\n",
    "\n",
    "同tensor.topk\n",
    "\n",
    "<u>torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)<u>\n",
    "\n",
    "Returns the k largest elements of the given input tensor along **a given dimension**. 维数不会变化，但是dim维维度减为k\n",
    "\n",
    "If **dim is not given**, the **last dimension** of the input is chosen.\n",
    "\n",
    "If largest is False then the k smallest elements are returned.\n",
    "\n",
    "A namedtuple of (values, indices) is returned, where the indices are the indices of the elements in the original input tensor.\n",
    "\n",
    "The boolean option sorted if True, will make sure that the returned k elements are themselves sorted\n",
    "\n",
    "### Torch.flatten\n",
    "\n",
    "<u>torch.flatten(input, start_dim=0, end_dim=- 1) → Tensor<u>\n",
    "\n",
    "Flattens input by reshaping it into a one-dimensional tensor. If **start_dim** or **end_dim** are passed, only dimensions starting with **start_dim** and ending with **end_dim** are flattened. The order of elements in input is unchanged.\n",
    "\n",
    "Unlike NumPy’s flatten, which always copies input’s data, this function may return the original object, a view, or copy. If no dimensions are flattened, then the original object input is returned. Otherwise, if input can be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the flattened shape is input’s data copied. See torch.Tensor.view() for details on when a view will be returned.\n",
    "\n",
    "NOTE\n",
    "\n",
    "<font color=red>Flattening a zero-dimensional tensor will return a one-dimensional view.</font>\n",
    "\n",
    "### Torch.arange\n",
    "<u>torch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor<u>\n",
    "    \n",
    "跟python里面的range()一样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde1b52",
   "metadata": {},
   "source": [
    "### Torch.gather\n",
    "\n",
    "<u>torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor<u>\n",
    "\n",
    "Gathers values along an axis specified by dim.\n",
    "\n",
    "For a 3-D tensor the output is specified by:\n",
    "\n",
    "out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
    "    \n",
    "out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
    "\n",
    "out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
    "\n",
    "input and index must have the same number of dimensions. It is also required that index.size(d) <= input.size(d) \n",
    "                                                                                                               \n",
    "for all dimensions d != dim. out will have the same shape as index. Note that input and index do not broadcast against each other.\n",
    "\n",
    "Parameters\n",
    "                                                                                                               input (Tensor) – the source tensor\n",
    "\n",
    "dim (int) – the axis along which to index\n",
    "\n",
    "index (LongTensor) – the indices of elements to gather\n",
    "                                                                                                                                                                                                                              \n",
    "                                                                                                               gather是从input里面取数，scatter是将src的数填入到self中，他们都有index。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b56ca0",
   "metadata": {},
   "source": [
    "### Torch.gt\n",
    "\n",
    "<u>torch.gt(input, other, *, out=None) → Tensor<u>\n",
    "\n",
    "Computes **input > other** element-wise.(gt=greater than)\n",
    "\n",
    "The second argument can be a number or a tensor whose shape is **broadcastable** with the first argument.\n",
    "\n",
    "Parameters\n",
    "    \n",
    "input (Tensor) – the tensor to compare\n",
    "\n",
    "other (Tensor or float) – the tensor or value to compare\n",
    "\n",
    "Returns\n",
    "A boolean tensor that is True where input is greater than other and False elsewhere\n",
    "\n",
    "Example:\n",
    "\n",
    ">torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\n",
    "    \n",
    ">tensor([[False, True], [False, False]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ff628",
   "metadata": {},
   "source": [
    "### Torch.where\n",
    "\n",
    "<u>torch.where(condition, x, y) → Tensor<u>\n",
    "\n",
    "Return a tensor of elements selected from either x or y, depending on condition.\n",
    "\n",
    "if condition_i:output_i=x_i;\n",
    "else output_i=y_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e311a72",
   "metadata": {},
   "source": [
    "### Torch.var\n",
    "\n",
    "<u>torch.var(input, dim, unbiased, keepdim=False, *, out=None) → Tensor<u>\n",
    "    \n",
    "计算方差\n",
    "    \n",
    "If unbiased is True, Bessel’s correction will be used. Otherwise, the sample variance is calculated, without any correction.\n",
    "\n",
    "Parameters\n",
    "    \n",
    "input (Tensor) – the input tensor.\n",
    "\n",
    "dim (int or tuple of python:ints) – the dimension or dimensions to reduce.\n",
    "\n",
    "Keyword Arguments\n",
    "    \n",
    "unbiased (bool) – whether to use Bessel’s correction (\\delta N = 1δN=1).\n",
    "\n",
    "keepdim (bool) – whether the output tensor has dim retained or not.\n",
    "\n",
    "out (Tensor, optional) – the output tensor.\n",
    "\n",
    "<u>torch.var(input, unbiased) → Tensor<u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d7c2f",
   "metadata": {},
   "source": [
    "### Torch.nonzero\n",
    "<u>torch.nonzero(input, *, out=None, as_tuple=False) → LongTensor or tuple of LongTensors<u>\n",
    "\n",
    "返回input中所有不为0的值的坐标。\n",
    "as_tuple=false时，返回一个二维矩阵，0维的维度是input中非0值的个数，1维的维度是input的维数（坐标）。\n",
    "as_tuple=true时，返回一个tensor的tuple，tuple[i]表示input中所有非0值第i维的坐标。\n",
    "    \n",
    "> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\n",
    "    \n",
    "> tensor([[ 0],\n",
    "        [ 1],\n",
    "        [ 2],\n",
    "        [ 4]])\n",
    "  \n",
    "> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.4, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 1.2, 0.0],\n",
    "                              [0.0, 0.0, 0.0,-0.4]]))\n",
    "    \n",
    "> tensor([[ 0,  0],\n",
    "         [ 1,  1],\n",
    "         [ 2,  2],\n",
    "         [ 3,  3]])\n",
    "  \n",
    "> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n",
    "\n",
    ">(tensor([0, 1, 2, 4]),)\n",
    "  \n",
    "> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.4, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 1.2, 0.0],\n",
    "                              [0.0, 0.0, 0.0,-0.4]]), as_tuple=True) \n",
    "    \n",
    ">(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491feb49",
   "metadata": {},
   "source": [
    "### TORCH.count_nonzero\n",
    "\n",
    "<u>torch.count_nonzero(input, dim=None) → Tensor<u>\n",
    "    \n",
    "Counts the number of non-zero values in the tensor input along the given dim. If no dim is specified then all non-zeros in the tensor are counted.\n",
    "\n",
    "Parameters\n",
    "    \n",
    "input (Tensor) – the input tensor.\n",
    "\n",
    "dim (int or tuple of python:ints, optional) – Dim or tuple of dims along which to count non-zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d225732",
   "metadata": {},
   "source": [
    "### Torch.squeeze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bec38e",
   "metadata": {},
   "source": [
    "### Torch.split\n",
    "\n",
    "<u>torch.split(tensor, split_size_or_sections, dim=0)<u>\n",
    "    \n",
    "Splits the tensor into chunks. Each chunk is a view of the original tensor.\n",
    "\n",
    "If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.\n",
    "\n",
    "If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections.(split_size_or_sections.sum()=tensor.shape[dim])\n",
    "\n",
    "> a=tensor([[0, 1],\n",
    "        [2, 3],\n",
    "        [4, 5],\n",
    "        [6, 7],\n",
    "        [8, 9]])\n",
    "    \n",
    "> torch.split(a, 2)\n",
    "    \n",
    "> (tensor([[0, 1],\n",
    "           [2, 3]]),\n",
    "   tensor([[4, 5],\n",
    "           [6, 7]]),\n",
    "   tensor([[8, 9]]))\n",
    "\n",
    "> torch.split(a, [1,4])\n",
    "    \n",
    "> (tensor([[0, 1]]),\n",
    "   tensor([[2, 3],\n",
    "           [4, 5],\n",
    "           [6, 7],\n",
    "           [8, 9]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc8267",
   "metadata": {},
   "source": [
    "###  Torch.sort\n",
    "\n",
    "<u>torch.sort(input, dim=- 1, descending=False, stable=False, *, out=None)<u>\n",
    "    \n",
    "Sorts the elements of the input tensor along a given dimension in ascending order by value.\n",
    "\n",
    "If **dim** is not given, the last dimension of the input is chosen.\n",
    "\n",
    "If **descending** is True then the elements are sorted in descending order by value.\n",
    "\n",
    "If **stable** is True then the sorting routine becomes stable, preserving the order of equivalent elements.\n",
    "\n",
    "A namedtuple of (values, indices) is returned, where the values are the sorted values and indices are the indices of the elements in the original input tensor.\n",
    "\n",
    "设定的dim就是所有值在这一维上排序，与其他维无关。默认是最后一维。返回排好序的tensor，以及对应的坐标。\n",
    "\n",
    ">a=torch.tensor([[8,2],\n",
    "                 [3,7]])\n",
    ">a.sort(0)\n",
    ">values=tensor([3,2],\n",
    "               [8,7])\n",
    ">index=tensor([1,0],\n",
    "              [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dbc561",
   "metadata": {},
   "source": [
    "### Torch.narrow\n",
    "\n",
    "<u>torch.narrow(input, dim, start, length) → Tensor<u>\n",
    "    \n",
    "Returns a new tensor that is a narrowed version of input tensor. The dimension dim is input from start to start + length. The returned tensor and input tensor share the same underlying storage.\n",
    "\n",
    "Parameters\n",
    "    \n",
    "input (Tensor) – the tensor to narrow\n",
    "\n",
    "dim (int) – the dimension along which to narrow\n",
    "\n",
    "start (int) – the starting dimension\n",
    "\n",
    "length (int) – the distance to the ending dimension\n",
    "\n",
    "参数里是length，不是end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963418eb",
   "metadata": {},
   "source": [
    "## Torch.Tensor\n",
    "\n",
    "### Torch.Tensor.scatter_\n",
    "\n",
    "<u>Tensor.scatter_(dim, index, src, reduce=None) → Tensor<u>\n",
    "\n",
    "与Tensor.Tensor.scatter相同\n",
    "\n",
    "Writes all values from the tensor **src** into **self** at the indices specified in the **index** tensor. \n",
    "\n",
    "对于所有src中的值src[i][j][k]坐标为(i,j,k)，将dim维替换成index[i][j][k],假设dim=2，得到新坐标(i,index[i][j][k],k)；在self中新坐标位置，也就是self[i][index[i][j][k]][k]位置填入src[i][j][k].\n",
    "\n",
    "所以shape(index)=shape(src),一一对应；\n",
    "max(index)<=self.shape[dim],替换的坐标不能超过这一维的维度\n",
    "\n",
    "scatter可用于构建onehot函数；与topk可能经常配套使用，因为topk返回前k个值和坐标，可以通过scatter构建只有topk有权值，其他地方为0的权值矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1bd1e1",
   "metadata": {},
   "source": [
    "### Torch.Tensor.index_add_\n",
    "\n",
    "<u>Tensor.index_add_(dim, index, tensor, *, alpha=1) → Tensor<u>\n",
    "\n",
    "指定tensor行或列的内容与self相加.\n",
    "index.shape[dim]=tensor[dim].\n",
    "dim=0时，self[index[i]]+=tensor[i]\n",
    "\n",
    "Accumulate the elements of **alpha** times **tensor** into the **self tensor** by adding to the indices in the order given in **index**. \n",
    "\n",
    "For example, if dim == 0, index[i] == j, and alpha=-1, then the ith row of tensor is subtracted from the jth row of self.\n",
    "\n",
    "The dim_th dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.\n",
    "\n",
    "Parameters\n",
    "\n",
    "dim (int) – dimension along which to index\n",
    "\n",
    "index (IntTensor or LongTensor) – indices of tensor to select from\n",
    "\n",
    "tensor (Tensor) – the tensor containing values to add\n",
    "\n",
    "Keyword Arguments\n",
    "\n",
    "alpha (Number) – the scalar multiplier for tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b62ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "b= tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "c= tensor([[ 3.,  9.,  5.,  1.,  7.],\n",
      "        [11., 17., 13.,  1., 15.],\n",
      "        [19., 25., 21.,  1., 23.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.ones(3,5)\n",
    "index=torch.tensor([0,2,4,1])\n",
    "b=torch.tensor(range(1,13)).reshape(3,4).float()\n",
    "print(\"a=\",a)\n",
    "print(\"b=\",b)\n",
    "c=a.index_add_(1,index,b,alpha=2)\n",
    "print(\"c=\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab53bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
