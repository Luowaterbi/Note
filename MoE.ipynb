{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f62347",
   "metadata": {
    "code_folding": [
     22,
     65
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "\n",
    "# constants\n",
    "\n",
    "MIN_EXPERT_CAPACITY = 4\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def default(val, default_val):\n",
    "    default_val = default_val() if isfunction(default_val) else default_val\n",
    "    return val if val is not None else default_val\n",
    "\n",
    "def cast_tuple(el):\n",
    "    return el if isinstance(el, tuple) else (el,)\n",
    "\n",
    "# tensor related helper functions\n",
    "\n",
    "def top1(t):\n",
    "    # 最后一维的第一大\n",
    "    values, index = t.topk(k=1, dim=-1)\n",
    "    # map(function,iterable)map() 会根据提供的函数对指定序列做映射。\n",
    "    # 第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。\n",
    "    # squeeze去掉dim维，这一维的维度必须为1，相当于降维，去掉维度为1的没用的维度\n",
    "    values, index = map(lambda x: x.squeeze(dim=-1), (values, index))\n",
    "    return values, index\n",
    "\n",
    "def cumsum_exclusive(t, dim=-1):\n",
    "    num_dims = len(t.shape)\n",
    "    # 算出dim后面有几维，因为pad的时候是从后往前，dim后面的不扩充，只有dim向上扩充一维\n",
    "    num_pad_dims = - dim - 1\n",
    "    pre_padding = (0, 0) * num_pad_dims\n",
    "    # slice(stop)/slice(start,stop[,step]) 切片函数，slice(5)切[0,5)，slice(1,5)切[1,5)\n",
    "    pre_slice   = (slice(None),) * num_pad_dims\n",
    "    # cumsum 根据指定的维度从前向后累加（不是sum，返回的是[a1,a2+a1,a3+a2+a1,……]）\n",
    "    # pad填充输入 F.pad(t,(0,0,1,0))意思是最后一维向左扩充0维，向右扩充0维；倒数第二维向上扩充1维，向下扩充0维。以此类推。\n",
    "    # pre_padding的作用体现在这里，从后往前扩充，没到dim都不扩充，到dim了往上走一维\n",
    "    padded_t = F.pad(t, (*pre_padding, 1, 0)).cumsum(dim=dim)\n",
    "    # 切掉dim的最后一维度。上面扩充了0，下面为什么要把总和切了？\n",
    "    return padded_t[(..., slice(None, -1), *pre_slice)]\n",
    "\n",
    "# pytorch one hot throws an error if there are out of bound indices.\n",
    "# tensorflow, in contrast, does not throw an error\n",
    "def safe_one_hot(indexes, max_length):\n",
    "    max_index = indexes.max() + 1\n",
    "    return F.one_hot(indexes, max(max_index + 1, max_length))[..., :max_length]\n",
    "\n",
    "def init_(t):\n",
    "    dim = t.shape[-1]\n",
    "    std = 1 / math.sqrt(dim)\n",
    "    return t.uniform_(-std, std)\n",
    "\n",
    "# activations\n",
    "\n",
    "class GELU_(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "GELU = nn.GELU if hasattr(nn, 'GELU') else GELU_\n",
    "\n",
    "# expert class\n",
    "\n",
    "class Experts(nn.Module):\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        num_experts = 16,\n",
    "        hidden_dim = None,\n",
    "        activation = GELU):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = default(hidden_dim, dim * 4)\n",
    "        num_experts = cast_tuple(num_experts)\n",
    "\n",
    "        w1 = torch.zeros(*num_experts, dim, hidden_dim)\n",
    "        w2 = torch.zeros(*num_experts, hidden_dim, dim)\n",
    "\n",
    "        w1 = init_(w1)\n",
    "        w2 = init_(w2)\n",
    "\n",
    "        self.w1 = nn.Parameter(w1)\n",
    "        self.w2 = nn.Parameter(w2)\n",
    "        self.act = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.einsum('...nd,...dh->...nh', x, self.w1)\n",
    "        hidden = self.act(hidden)\n",
    "        out    = torch.einsum('...nh,...hd->...nd', hidden, self.w2)\n",
    "        return out\n",
    "\n",
    "# the below code is almost all transcribed from the official tensorflow version, from which the papers are written\n",
    "# https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/moe.py\n",
    "\n",
    "# gating network\n",
    "\n",
    "class Top2Gating(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_gates,\n",
    "        eps = 1e-9,\n",
    "        outer_expert_dims = tuple(),\n",
    "        second_policy_train = 'random',\n",
    "        second_policy_eval = 'random',\n",
    "        second_threshold_train = 0.2,\n",
    "        second_threshold_eval = 0.2,\n",
    "        capacity_factor_train = 1.25,\n",
    "        capacity_factor_eval = 2.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.num_gates = num_gates\n",
    "        # w_gating维度是 outer_expert_dims * dim * num_gates\n",
    "        # 输出是门数，所以最后一维是num_gates\n",
    "        # dim是特征维数，作为中间维度与输入相乘\n",
    "        # outer_experts_dims是啥？\n",
    "        self.w_gating = nn.Parameter(torch.randn(*outer_expert_dims, dim, num_gates))\n",
    "\n",
    "        self.second_policy_train = second_policy_train\n",
    "        self.second_policy_eval = second_policy_eval\n",
    "        self.second_threshold_train = second_threshold_train\n",
    "        self.second_threshold_eval = second_threshold_eval\n",
    "        self.capacity_factor_train = capacity_factor_train\n",
    "        self.capacity_factor_eval = capacity_factor_eval\n",
    "\n",
    "    def forward(self, x, importance = None):\n",
    "        *_, b, group_size, dim = x.shape\n",
    "        num_gates = self.num_gates\n",
    "\n",
    "        # 这应该是module自带参数，train（）的时候为True，eval（）的时候为负\n",
    "        if self.training:\n",
    "            policy = self.second_policy_train\n",
    "            threshold = self.second_threshold_train\n",
    "            capacity_factor = self.capacity_factor_train\n",
    "        else:\n",
    "            policy = self.second_policy_eval\n",
    "            threshold = self.second_threshold_eval\n",
    "            capacity_factor = self.capacity_factor_eval\n",
    "\n",
    "        raw_gates = torch.einsum('...bnd,...de->...bne', x, self.w_gating)\n",
    "        # 最后一维是gate，所以按照最后一维softmax\n",
    "        raw_gates = raw_gates.softmax(dim=-1)\n",
    "\n",
    "        # FIND TOP 2 EXPERTS PER POSITON\n",
    "        # Find the top expert for each position. shape=[batch, group]\n",
    "\n",
    "        gate_1, index_1 = top1(raw_gates)\n",
    "        mask_1 = F.one_hot(index_1, num_gates).float()\n",
    "        density_1_proxy = raw_gates\n",
    "        \n",
    "        # 默认不用L_importance\n",
    "        if importance is not None:\n",
    "            equals_one_mask = (importance == 1.).float()\n",
    "            mask_1 *= equals_one_mask[..., None]\n",
    "            gate_1 *= equals_one_mask\n",
    "            density_1_proxy *= equals_one_mask[..., None]\n",
    "            del equals_one_mask\n",
    "\n",
    "        # mask_1是top1下标的one-hot编码，1-mask_1也就意味着top1对应的位置*0，其他的位置*1\n",
    "        # 变成0了就是最小的了，再一次top1就可以提取出top2了    \n",
    "        gates_without_top_1 = raw_gates * (1. - mask_1)\n",
    "\n",
    "        gate_2, index_2 = top1(gates_without_top_1)\n",
    "        mask_2 = F.one_hot(index_2, num_gates).float()\n",
    "\n",
    "        if importance is not None:\n",
    "            greater_zero_mask = (importance > 0.).float()\n",
    "            mask_2 *= greater_zero_mask[..., None]\n",
    "            del greater_zero_mask\n",
    "\n",
    "        # normalize top2 gate scores\n",
    "        #只保留最大的2个，这里相当于对最大的两个又进行一次softmax\n",
    "        denom = gate_1 + gate_2 + self.eps\n",
    "        gate_1 /= denom\n",
    "        gate_2 /= denom\n",
    "\n",
    "        # BALANCING LOSSES\n",
    "        # shape = [batch, experts]\n",
    "        # We want to equalize the fraction of the batch assigned to each expert\n",
    "        density_1 = mask_1.mean(dim=-2) # [batch_size, expert_num]每个expert在每个batch里被选中的平均次数\n",
    "        # Something continuous that is correlated with what we want to equalize.\n",
    "        # 没看懂怎么算的\n",
    "        density_1_proxy = density_1_proxy.mean(dim=-2) # [batch_size, expert_num]每个expert在每个batch里softmax后平均数值\n",
    "        loss = (density_1_proxy * density_1).mean() * float(num_gates ** 2)\n",
    "\n",
    "        # Depending on the policy in the hparams, we may drop out some of the\n",
    "        # second-place experts.\n",
    "        # 对第二expert进行类似dropout的操作，可能删去低于阈值的，可能随机扔一些，可能都不要，可能都要\n",
    "        if policy == \"all\":\n",
    "            pass\n",
    "        elif policy == \"none\":\n",
    "            mask_2 = torch.zeros_like(mask_2)\n",
    "        elif policy == \"threshold\":\n",
    "            mask_2 *= (gate_2 > threshold).float()\n",
    "        elif policy == \"random\":\n",
    "            # uniform_在均匀分布中随机采样，左闭右开\n",
    "            probs = torch.zeros_like(gate_2).uniform_(0., 1.) #[inputs[0], outer_expert_dim]\n",
    "            # 是mask_2*=，不是=，unsqueeze加一维\n",
    "            mask_2 *= (probs < (gate_2 / max(threshold, self.eps))).float().unsqueeze(-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown policy {policy}\")\n",
    "\n",
    "        # Each sequence sends (at most?) expert_capacity positions to each expert.\n",
    "        # Static expert_capacity dimension is needed for expert batch sizes\n",
    "        expert_capacity = min(group_size, int((group_size * capacity_factor) / num_gates))\n",
    "        expert_capacity = max(expert_capacity, MIN_EXPERT_CAPACITY)\n",
    "        expert_capacity_f = float(expert_capacity)\n",
    "\n",
    "        # COMPUTE ASSIGNMENT TO EXPERTS\n",
    "        # [batch, group, experts]\n",
    "        # This is the position within the expert's mini-batch for this sequence\n",
    "        position_in_expert_1 = cumsum_exclusive(mask_1, dim=-2) * mask_1\n",
    "        # Remove the elements that don't fit. [batch, group, experts]\n",
    "        mask_1 *= (position_in_expert_1 < expert_capacity_f).float()\n",
    "        # [batch, experts]\n",
    "        # How many examples in this sequence go to this expert\n",
    "        mask_1_count = mask_1.sum(dim=-2, keepdim=True)\n",
    "        # [batch, group] - mostly ones, but zeros where something didn't fit\n",
    "        mask_1_flat = mask_1.sum(dim=-1)\n",
    "        # [batch, group]\n",
    "        position_in_expert_1 = position_in_expert_1.sum(dim=-1)\n",
    "        # Weight assigned to first expert.  [batch, group]\n",
    "        gate_1 *= mask_1_flat\n",
    "\n",
    "        # 加上mask_1_count,是算每个expert一共被加了几次，不能超过expert_capacity\n",
    "        position_in_expert_2 = cumsum_exclusive(mask_2, dim=-2) + mask_1_count\n",
    "        position_in_expert_2 *= mask_2\n",
    "        mask_2 *= (position_in_expert_2 < expert_capacity_f).float()\n",
    "        mask_2_flat = mask_2.sum(dim=-1)\n",
    "\n",
    "        position_in_expert_2 = position_in_expert_2.sum(dim=-1)\n",
    "        gate_2 *= mask_2_flat\n",
    "        \n",
    "        # [batch, group, experts, expert_capacity]\n",
    "        # None就是这一维的里面加一维\n",
    "        # F.one_hot及之前的累乘[batch,group,expert,1],safe_one_hot是[batch,group,1,expert_capacity]\n",
    "        combine_tensor = (\n",
    "            gate_1[..., None, None]\n",
    "            * mask_1_flat[..., None, None]\n",
    "            * F.one_hot(index_1, num_gates)[..., None]\n",
    "            * safe_one_hot(position_in_expert_1.long(), expert_capacity)[..., None, :] +\n",
    "            gate_2[..., None, None]\n",
    "            * mask_2_flat[..., None, None]\n",
    "            * F.one_hot(index_2, num_gates)[..., None]\n",
    "            * safe_one_hot(position_in_expert_2.long(), expert_capacity)[..., None, :]\n",
    "        )\n",
    "\n",
    "        dispatch_tensor = combine_tensor.bool().to(combine_tensor)\n",
    "        return dispatch_tensor, combine_tensor, loss\n",
    "\n",
    "# plain mixture of experts\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        num_experts = 16,\n",
    "        hidden_dim = None,\n",
    "        activation = nn.ReLU,\n",
    "        second_policy_train = 'random',\n",
    "        second_policy_eval = 'random',\n",
    "        second_threshold_train = 0.2,\n",
    "        second_threshold_eval = 0.2,\n",
    "        capacity_factor_train = 1.25,\n",
    "        capacity_factor_eval = 2.,\n",
    "        loss_coef = 1e-2,\n",
    "        experts = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        gating_kwargs = {'second_policy_train': second_policy_train, 'second_policy_eval': second_policy_eval, 'second_threshold_train': second_threshold_train, 'second_threshold_eval': second_threshold_eval, 'capacity_factor_train': capacity_factor_train, 'capacity_factor_eval': capacity_factor_eval}\n",
    "        # 初始化gate，传入门的数量，和超参，没有传入outer_expert_dims\n",
    "        self.gate = Top2Gating(dim, num_gates = num_experts, **gating_kwargs)\n",
    "        self.experts = default(experts, lambda: Experts(dim, num_experts = num_experts, hidden_dim = hidden_dim, activation = activation))\n",
    "        self.loss_coef = loss_coef\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        b, n, d, e = *inputs.shape, self.num_experts\n",
    "        dispatch_tensor, combine_tensor, loss = self.gate(inputs)\n",
    "        expert_inputs = torch.einsum('bnd,bnec->ebcd', inputs, dispatch_tensor)\n",
    "\n",
    "        # Now feed the expert inputs through the experts.\n",
    "        orig_shape = expert_inputs.shape\n",
    "        expert_inputs = expert_inputs.reshape(e, -1, d)\n",
    "        expert_outputs = self.experts(expert_inputs)\n",
    "        expert_outputs = expert_outputs.reshape(*orig_shape)\n",
    "\n",
    "        output = torch.einsum('ebcd,bnec->bnd', expert_outputs, combine_tensor)\n",
    "        return output, loss * self.loss_coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609988e0",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from mixture_of_experts import MoE\n",
    "\n",
    "moe = MoE(\n",
    "    dim = 512,\n",
    "    num_experts = 16,               # increase the experts (# parameters) of your model without increasing computation\n",
    "    hidden_dim = 512 * 4,           # size of hidden dimension in each expert, defaults to 4 * dimension\n",
    "    activation = nn.LeakyReLU,      # use your preferred activation, will default to GELU\n",
    "    second_policy_train = 'random', # in top_2 gating, policy for whether to use a second-place expert\n",
    "    second_policy_eval = 'random',  # all (always) | none (never) | threshold (if gate value > the given threshold) | random (if gate value > threshold * random_uniform(0, 1))\n",
    "    second_threshold_train = 0.2,\n",
    "    second_threshold_eval = 0.2,\n",
    "    capacity_factor_train = 1.25,   # experts have fixed capacity per batch. we need some extra capacity in case gating is not perfectly balanced.\n",
    "    capacity_factor_eval = 2.,      # capacity_factor_* should be set to a value >=1\n",
    "    loss_coef = 1e-2                # multiplier on the auxiliary expert balancing auxiliary loss\n",
    ")\n",
    "\n",
    "inputs = torch.randn(4, 1024, 512)\n",
    "out, aux_loss = moe(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3a666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-0.9714, -0.0394, -0.1725, -0.9955, -0.4789],\n",
      "         [ 0.2099,  0.3940, -0.6335,  2.1337, -0.3239],\n",
      "         [-1.1338, -1.1216, -0.1250,  0.1894, -0.6138],\n",
      "         [-0.4100, -1.2906,  2.0499,  2.3571,  0.1728]],\n",
      "\n",
      "        [[ 0.3384, -0.2154, -0.0921,  0.9377, -1.2150],\n",
      "         [-0.3789,  0.1804,  0.9904, -0.3555, -0.4614],\n",
      "         [ 2.1593,  0.1664,  1.2265,  0.4760,  0.5301],\n",
      "         [ 0.7857, -0.1060,  0.7419,  0.5517, -0.1416]],\n",
      "\n",
      "        [[ 1.0148,  2.0236,  0.7502, -2.2973,  0.0156],\n",
      "         [-0.8988,  0.4086, -0.4696, -0.5865,  0.2939],\n",
      "         [ 1.4902, -0.0257,  0.1126,  0.8801, -0.9802],\n",
      "         [-1.9549,  2.3498, -0.0437, -0.0219, -0.6894]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "outer_expert_dims=tuple([3])\n",
    "dim=4\n",
    "num_gates=5\n",
    "w_gating = nn.Parameter(torch.randn(*outer_expert_dims, dim, num_gates))\n",
    "print(w_gating)\n",
    "# 将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter绑定到这个module里面\n",
    "# (net.parameter()中就有这个绑定的parameter，所以在参数优化的时候可以进行优化的)\n",
    "# 所以经过类型转换变成了模型的一部分，成为了模型中根据训练可以改动的参数了\n",
    "# 使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f1dcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs= tensor([[-0.9795,  0.1923, -0.6336,  0.3192],\n",
      "        [ 0.5834,  0.5061,  0.9854, -0.9274],\n",
      "        [-1.9581,  1.6119,  0.5217,  0.2567]])\n",
      "raw_gates= tensor([[[ 1.5793,  0.4130,  0.7807,  2.0178,  0.8509],\n",
      "         [-1.5216,  0.1064, -0.2597, -1.1123,  0.7202],\n",
      "         [-2.7351, -1.1371, -0.9104,  1.5727,  0.4423]],\n",
      "\n",
      "        [[-1.1975,  0.2681, -2.4454, -1.5001, -1.2085],\n",
      "         [ 1.4049,  0.2280,  0.9682,  0.3245, -0.2886],\n",
      "         [ 3.4187, -0.8172,  0.3514, -0.7495, -0.1687]],\n",
      "\n",
      "        [[ 1.5438, -0.2042, -0.2223,  6.0924,  0.1399],\n",
      "         [ 0.0548,  0.7722,  2.6070, -2.0191,  1.8755],\n",
      "         [-3.1603, -2.7140, -2.1785,  4.0064, -0.2450]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "raw_gates= tensor([[[2.6355e-01, 8.2095e-02, 1.1859e-01, 4.0857e-01, 1.2720e-01],\n",
      "         [4.8679e-02, 2.4796e-01, 1.7195e-01, 7.3303e-02, 4.5811e-01],\n",
      "         [9.0573e-03, 4.4773e-02, 5.6165e-02, 6.7278e-01, 2.1722e-01]],\n",
      "\n",
      "        [[1.3615e-01, 5.8951e-01, 3.9086e-02, 1.0060e-01, 1.3466e-01],\n",
      "         [4.0359e-01, 1.2440e-01, 2.6078e-01, 1.3701e-01, 7.4213e-02],\n",
      "         [9.0566e-01, 1.3103e-02, 4.2159e-02, 1.4021e-02, 2.5060e-02]],\n",
      "\n",
      "        [[1.0407e-02, 1.8121e-03, 1.7796e-03, 9.8345e-01, 2.5562e-03],\n",
      "         [4.5075e-02, 9.2366e-02, 5.7852e-01, 5.6654e-03, 2.7838e-01],\n",
      "         [7.5799e-04, 1.1844e-03, 2.0232e-03, 9.8205e-01, 1.3987e-02]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs=torch.randn(3,4)\n",
    "print(\"inputs=\",inputs)\n",
    "# *_, b, group_size, dim = inputs.shape\n",
    "# print(\"_,b,group_size,dim=\",_,b,group_size,dim)\n",
    "raw_gates = torch.einsum('ad,bde->abe', inputs, w_gating)\n",
    "print(\"raw_gates=\",raw_gates)\n",
    "raw_gates = raw_gates.softmax(dim=-1)\n",
    "print(\"raw_gates=\",raw_gates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8d4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_1,index_1=top1(raw_gates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8885ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4086, 0.4581, 0.6728],\n",
      "        [0.5895, 0.4036, 0.9057],\n",
      "        [0.9834, 0.5785, 0.9820]], grad_fn=<SqueezeBackward1>) tensor([[3, 4, 3],\n",
      "        [1, 0, 0],\n",
      "        [3, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(gate_1,index_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da25aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4086],\n",
      "         [0.4581],\n",
      "         [0.6728]],\n",
      "\n",
      "        [[0.5895],\n",
      "         [0.4036],\n",
      "         [0.9057]],\n",
      "\n",
      "        [[0.9834],\n",
      "         [0.5785],\n",
      "         [0.9820]]], grad_fn=<TopkBackward0>) tensor([[[3],\n",
      "         [4],\n",
      "         [3]],\n",
      "\n",
      "        [[1],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[3],\n",
      "         [2],\n",
      "         [3]]])\n",
      "tensor([[0.4086, 0.4581, 0.6728],\n",
      "        [0.5895, 0.4036, 0.9057],\n",
      "        [0.9834, 0.5785, 0.9820]], grad_fn=<SqueezeBackward1>) tensor([[3, 4, 3],\n",
      "        [1, 0, 0],\n",
      "        [3, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "values, index = raw_gates.topk(k=1, dim=-1)\n",
    "print(values, index)\n",
    "values, index = map(lambda x: x.squeeze(dim=-1), (values, index))\n",
    "print(values, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30b2326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.]]])\n",
      "tensor([[[0.2635, 0.0821, 0.1186, 0.0000, 0.1272],\n",
      "         [0.0487, 0.2480, 0.1719, 0.0733, 0.0000],\n",
      "         [0.0091, 0.0448, 0.0562, 0.0000, 0.2172]],\n",
      "\n",
      "        [[0.1361, 0.0000, 0.0391, 0.1006, 0.1347],\n",
      "         [0.0000, 0.1244, 0.2608, 0.1370, 0.0742],\n",
      "         [0.0000, 0.0131, 0.0422, 0.0140, 0.0251]],\n",
      "\n",
      "        [[0.0104, 0.0018, 0.0018, 0.0000, 0.0026],\n",
      "         [0.0451, 0.0924, 0.0000, 0.0057, 0.2784],\n",
      "         [0.0008, 0.0012, 0.0020, 0.0000, 0.0140]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask_1 = F.one_hot(index_1, num_gates).float()\n",
    "density_1_proxy = raw_gates\n",
    "print(mask_1)\n",
    "gates_without_top_1 = raw_gates * (1. - mask_1)\n",
    "print(gates_without_top_1)\n",
    "gate_2, index_2 = top1(gates_without_top_1)\n",
    "mask_2 = F.one_hot(index_2, num_gates).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ed41f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density_1= tensor([[0.0000, 0.0000, 0.0000, 0.6667, 0.3333],\n",
      "        [0.6667, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3333, 0.6667, 0.0000]])\n",
      "density_1_proxy= tensor([[0.1071, 0.1249, 0.1156, 0.3849, 0.2675],\n",
      "        [0.4818, 0.2423, 0.1140, 0.0839, 0.0780],\n",
      "        [0.0187, 0.0318, 0.1941, 0.6571, 0.0983]], grad_fn=<MeanBackward1>)\n",
      "loss= tensor(2.0841, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "density_1 = mask_1.mean(dim=-2)\n",
    "print(\"density_1=\", density_1)\n",
    "# Something continuous that is correlated with what we want to equalize.\n",
    "density_1_proxy = density_1_proxy.mean(dim=-2)\n",
    "print(\"density_1_proxy=\", density_1_proxy)\n",
    "loss = (density_1_proxy * density_1).mean() * float(num_gates ** 2)\n",
    "print(\"loss=\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2af62",
   "metadata": {},
   "source": [
    "~~1. outer_experts_dim 是什么？为什么初始化的时候不传入，初始为 tuple()~~\n",
    "2. loss到底是什么原理\n",
    "~~3. expert_capacity是干什么的~~\n",
    "4. cumsum_exclusive为什么dim向上扩充1层0，最后返回切掉代表总和的最后1层？\n",
    "5. compute assignment to experts是什么作用？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f00ed717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs= tensor([[0.9989, 0.2530, 0.9471],\n",
      "        [0.3869, 0.5935, 0.5862],\n",
      "        [0.6400, 0.5997, 0.4616]])\n",
      "drop= tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [1.],\n",
      "         [0.]]])\n",
      "gate_2= tensor([[0.2635, 0.2480, 0.2172],\n",
      "        [0.1361, 0.2608, 0.0422],\n",
      "        [0.0104, 0.2784, 0.0140]], grad_fn=<SqueezeBackward1>)\n",
      "mask_2 tensor([[[1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "probs = torch.zeros_like(gate_2).uniform_(0., 1.)\n",
    "print(\"probs=\",probs)\n",
    "print(\"drop=\",(probs < (gate_2 / max(0.2, 1e-9))).float().unsqueeze(-1))\n",
    "mask_2 *= (probs < (gate_2 / max(0.2, 1e-9))).float().unsqueeze(-1)\n",
    "print(\"gate_2=\", gate_2)\n",
    "print(\"mask_2\",mask_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74463338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_1= tensor([[[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.]]])\n",
      "return= tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 1., 0.]]])\n",
      "position_in_expert_1 tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "position_in_expert_1 = cumsum_exclusive(mask_1, dim=-2) * mask_1\n",
    "print(\"mask_1=\",mask_1)\n",
    "print(\"return=\",cumsum_exclusive(mask_1, dim=-2))\n",
    "print(\"position_in_expert_1\",position_in_expert_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "333d7db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_padding= (0, 0)\n",
      "pre_slice= (slice(None, None, None),)\n",
      "F.pad(t,(*pre_padding,1,0))= (0, 0, 1, 0)\n",
      "padded_t= tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 2., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [2., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 1., 0.],\n",
      "         [0., 0., 1., 2., 0.]]])\n",
      "slice(None,-1)= slice(None, -1, None)\n",
      "return= tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 1., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "t=mask_1 # [batch_size,outer_expert_size,num_gates/experts]\n",
    "dim_cumsum=-2\n",
    "num_dims = len(t.shape)\n",
    "num_pad_dims = - dim_cumsum - 1 # 将dim变成正的，还是原来的那一维\n",
    "pre_padding = (0, 0) * num_pad_dims\n",
    "print(\"pre_padding=\",pre_padding)\n",
    "pre_slice   = (slice(None),) * num_pad_dims\n",
    "print(\"pre_slice=\",pre_slice)\n",
    "print(\"F.pad(t,(*pre_padding,1,0))=\",(*pre_padding, 1, 0))\n",
    "padded_t = F.pad(t, (*pre_padding, 1, 0)).cumsum(dim=dim_cumsum)\n",
    "print(\"padded_t=\",padded_t)\n",
    "print(\"slice(None,-1)=\",slice(None, -1))\n",
    "print(\"return=\",padded_t[(..., slice(None, -1), *pre_slice)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e608c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_slice= (slice(None, -1, None), slice(None, None, None))\n",
      "test_tensor= tensor([[[-1.4956, -0.3686,  0.0424, -0.5827,  1.0200],\n",
      "         [ 1.3004,  0.6205,  0.0248,  0.2213,  0.4334],\n",
      "         [-2.5851, -0.0540, -0.7103, -0.2518, -0.2877],\n",
      "         [ 0.3636, -0.3669, -0.8981, -0.0413, -1.2482]],\n",
      "\n",
      "        [[ 0.7614, -0.7035, -0.2184,  0.1117,  0.2142],\n",
      "         [-1.0687,  0.2303, -2.1075,  2.2519,  0.4521],\n",
      "         [-0.4342,  1.2900,  0.8189, -2.5383, -0.1784],\n",
      "         [-0.6281, -0.7641, -0.8560, -0.1367,  0.8297]],\n",
      "\n",
      "        [[ 0.5288, -0.2732,  0.0165, -0.4402,  0.1158],\n",
      "         [ 0.5225, -1.8376, -0.5805,  1.7009,  0.2643],\n",
      "         [-1.0354,  0.0363,  1.5778, -1.2066,  1.3709],\n",
      "         [-0.8472,  1.2676,  2.6247, -1.1441, -1.2538]]])\n",
      "slice= tensor([[[-1.4956, -0.3686,  0.0424, -0.5827,  1.0200],\n",
      "         [ 1.3004,  0.6205,  0.0248,  0.2213,  0.4334],\n",
      "         [-2.5851, -0.0540, -0.7103, -0.2518, -0.2877]],\n",
      "\n",
      "        [[ 0.7614, -0.7035, -0.2184,  0.1117,  0.2142],\n",
      "         [-1.0687,  0.2303, -2.1075,  2.2519,  0.4521],\n",
      "         [-0.4342,  1.2900,  0.8189, -2.5383, -0.1784]],\n",
      "\n",
      "        [[ 0.5288, -0.2732,  0.0165, -0.4402,  0.1158],\n",
      "         [ 0.5225, -1.8376, -0.5805,  1.7009,  0.2643],\n",
      "         [-1.0354,  0.0363,  1.5778, -1.2066,  1.3709]]])\n"
     ]
    }
   ],
   "source": [
    "all_slice=(slice(None, -1),*pre_slice)\n",
    "test_tensor=torch.randn(3,4,5)\n",
    "print(\"all_slice=\",all_slice)\n",
    "print(\"test_tensor=\",test_tensor)\n",
    "print(\"slice=\",test_tensor[(...,*all_slice)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b1d872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert_capacity= 0\n",
      "4\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "group_size = 3\n",
    "capacity_factor = 1.25\n",
    "expert_capacity = min(group_size, int((group_size * capacity_factor) / num_gates))\n",
    "print(\"expert_capacity=\",expert_capacity)\n",
    "expert_capacity = max(expert_capacity, MIN_EXPERT_CAPACITY)\n",
    "print(expert_capacity)\n",
    "expert_capacity_f = float(expert_capacity)\n",
    "print(expert_capacity_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "005b4df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_in_expert_1= tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.]]])\n",
      "mask_1= tensor([[[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.]]])\n",
      "mask_1_count= tensor([[[0., 0., 0., 2., 1.]],\n",
      "\n",
      "        [[2., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 2., 0.]]])\n",
      "mask_1_flat= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "position_in_expert_1= tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]])\n",
      "gate_1= tensor([[0.4086, 0.4581, 0.6728],\n",
      "        [0.5895, 0.4036, 0.9057],\n",
      "        [0.9834, 0.5785, 0.9820]], grad_fn=<AsStridedBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"position_in_expert_1=\",position_in_expert_1)\n",
    "mask_1 *= (position_in_expert_1 < expert_capacity_f).float()\n",
    "# [batch, experts]\n",
    "# How many examples in this sequence go to this expert\n",
    "print(\"mask_1=\", mask_1)\n",
    "mask_1_count = mask_1.sum(dim=-2, keepdim=True)\n",
    "print(\"mask_1_count=\", mask_1_count)\n",
    "# [batch, group] - mostly ones, but zeros where something didn't fit\n",
    "mask_1_flat = mask_1.sum(dim=-1)\n",
    "print(\"mask_1_flat=\",mask_1_flat)\n",
    "# [batch, group]\n",
    "position_in_expert_1 = position_in_expert_1.sum(dim=-1)\n",
    "print(\"position_in_expert_1=\",position_in_expert_1)\n",
    "# Weight assigned to first expert.  [batch, group]\n",
    "gate_1 *= mask_1_flat\n",
    "print(\"gate_1=\", gate_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c1c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.0308e-01, -2.0393e-02, -1.6784e+00, -1.4372e+00],\n",
      "         [ 4.4806e-02, -1.4272e+00, -9.8718e-02,  1.0854e-01],\n",
      "         [-9.0818e-01,  4.7623e-01,  1.2458e-03, -1.1817e+00]],\n",
      "\n",
      "        [[-7.9067e-01,  1.2508e+00,  1.8369e+00, -6.5970e-01],\n",
      "         [-1.9559e+00,  5.2007e-01, -5.7840e-01, -6.8399e-01],\n",
      "         [ 1.3251e+00, -8.9617e-02,  1.2529e+00, -1.4444e+00]]])\n",
      "tensor([[[-0.7907,  1.2508,  1.8369, -0.6597],\n",
      "         [-1.9559,  0.5201, -0.5784, -0.6840],\n",
      "         [ 1.3251, -0.0896,  1.2529, -1.4444]]])\n"
     ]
    }
   ],
   "source": [
    "aaaaa=torch.randn(2,3,4)\n",
    "print(aaaaa)\n",
    "a=slice(-1,None)\n",
    "print(aaaaa[a])\n",
    "# print(aaaaa[...,None,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa68f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "无",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
